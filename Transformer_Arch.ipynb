{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHMC2Sf3wC6Q6l7MIKLTMM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahzadahmad3/Natural-Language-Processing/blob/main/Transformer_Arch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "Up_N9o3tga5b"
      },
      "outputs": [],
      "source": [
        "#Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since Transformers don't process input sequentially, **positional encodings** are added to provide information about the position of each word in the sequence.\n",
        "\n",
        "Input=Embeddings+PositionalÂ Encoding\n"
      ],
      "metadata": {
        "id": "0bVCnOa7qHUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Represents a single encoder block in the Transformer architecture.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, dff, seq_length):\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "\n",
        "        self.pos_encoding = PositionalEncoding(seq_length, embed_dim)\n",
        "        self.multi_head_attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(embed_dim)\n",
        "        ])\n",
        "\n",
        "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Processes the input through the encoder block.\n",
        "        \"\"\"\n",
        "        x = self.pos_encoding(inputs)  # Add positional encoding\n",
        "\n",
        "        # Multi-Head Self-Attention\n",
        "        attn_output = self.multi_head_attention(x, x)\n",
        "        x = self.layer_norm1(x + attn_output)  # Add Residual Connection & LayerNorm\n",
        "\n",
        "        # Feed-Forward Network\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.layer_norm2(x + ffn_output)  # Add Residual Connection & LayerNorm\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "6oNm2ojPXlrA"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Creates positional encodings to inject sequence order information into the model.\n",
        "    \"\"\"\n",
        "    def __init__(self, seq_length, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_encoding = self.create_positional_encoding(seq_length, d_model)\n",
        "\n",
        "    def create_positional_encoding(self, seq_length, d_model):\n",
        "        \"\"\"\n",
        "        Calculates positional encodings using sine and cosine functions.\n",
        "        \"\"\"\n",
        "        # Generate position and dimension indices\n",
        "        pos_encoding = np.zeros((seq_length, d_model))\n",
        "        for pos in range(seq_length):\n",
        "            for i in range(d_model):\n",
        "                if i % 2 == 0:  # Even index\n",
        "                    pos_encoding[pos, i] = np.sin(pos / np.power(10000, (2 * (i // 2)) / d_model))\n",
        "                else:  # Odd index\n",
        "                    pos_encoding[pos, i] = np.cos(pos / np.power(10000, (2 * (i // 2)) / d_model))\n",
        "        # Convert to TensorFlow tensor\n",
        "        return tf.convert_to_tensor(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\"\n",
        "        Adds positional encodings to the input.\n",
        "        \"\"\"\n",
        "        return x + self.pos_encoding[:tf.shape(x)[1], :]"
      ],
      "metadata": {
        "id": "C2XTvG_-WsJ-"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderBlock(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Represents a single decoder block in the Transformer architecture.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, dff, seq_length):\n",
        "        super(TransformerDecoderBlock, self).__init__()\n",
        "\n",
        "        self.pos_encoding = PositionalEncoding(seq_length, embed_dim)\n",
        "        self.masked_multi_head_attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.cross_attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(embed_dim)\n",
        "        ])\n",
        "\n",
        "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layer_norm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs):\n",
        "        \"\"\"\n",
        "        Processes the input through the decoder block.\n",
        "        \"\"\"\n",
        "        x = self.pos_encoding(inputs)  # Add positional encoding\n",
        "\n",
        "        # Masked Multi-Head Self-Attention\n",
        "        attn_mask = self.create_causal_mask(tf.shape(x)[1])\n",
        "        attn_output = self.masked_multi_head_attention(x, x, attention_mask=attn_mask)\n",
        "        x = self.layer_norm1(x + attn_output)  # Add Residual Connection & LayerNorm\n",
        "\n",
        "        # Cross-Attention (Encoder-Decoder Attention)\n",
        "        cross_attn_output = self.cross_attention(x, encoder_outputs)\n",
        "        x = self.layer_norm2(x + cross_attn_output)  # Add Residual Connection & LayerNorm\n",
        "\n",
        "        # Feed-Forward Network\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.layer_norm3(x + ffn_output)  # Add Residual Connection & LayerNorm\n",
        "\n",
        "        return x\n",
        "\n",
        "    def create_causal_mask(self, seq_length):\n",
        "        \"\"\"\n",
        "        Creates a causal mask to prevent the decoder from attending to future tokens.\n",
        "        \"\"\"\n",
        "        mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)  # Lower triangular matrix\n",
        "        return tf.cast(mask, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "DDHUAeKVfZq7"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Implements the Transformer architecture.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, dff, num_layers, seq_length, vocab_size):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_encoding = PositionalEncoding(seq_length, embed_dim)\n",
        "\n",
        "        self.encoder_stack = [TransformerEncoderBlock(embed_dim, num_heads, dff, seq_length) for _ in range(num_layers)]\n",
        "        self.decoder_stack = [TransformerDecoderBlock(embed_dim, num_heads, dff, seq_length) for _ in range(num_layers)]\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, targets):\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the Transformer.\n",
        "        \"\"\"\n",
        "        # Step 1: Embedding and Positional Encoding\n",
        "        enc_input = self.pos_encoding(self.embedding(inputs))  # Encoder input\n",
        "        dec_input = self.pos_encoding(self.embedding(targets))  # Decoder input\n",
        "\n",
        "        # Step 2: Encoder Stack\n",
        "        enc_output = enc_input\n",
        "        for encoder_block in self.encoder_stack:\n",
        "            enc_output = encoder_block(enc_output)\n",
        "\n",
        "        # Step 3: Decoder Stack\n",
        "        dec_output = dec_input\n",
        "        for decoder_block in self.decoder_stack:\n",
        "            dec_output = decoder_block(dec_output, enc_output)\n",
        "\n",
        "        # Step 4: Final Linear Layer\n",
        "        logits = self.final_layer(dec_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "oOpdOl2whhbN"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "embed_dim = 64\n",
        "num_heads = 8\n",
        "dff = 256\n",
        "num_layers = 4\n",
        "seq_length = 10\n",
        "vocab_size = 5000  # Example vocabulary size\n",
        "\n",
        "transformer_model = Transformer(embed_dim, num_heads, dff, num_layers, seq_length, vocab_size)\n",
        "\n",
        "# Dummy input (Batch size = 2, Sequence length = 10)\n",
        "dummy_input = tf.random.uniform((2, seq_length), minval=0, maxval=vocab_size, dtype=tf.int32)\n",
        "dummy_target = tf.random.uniform((2, seq_length), minval=0, maxval=vocab_size, dtype=tf.int32)\n",
        "\n",
        "# Call the model with only the required arguments: inputs and targets\n",
        "output = transformer_model(dummy_input, dummy_target)\n",
        "\n",
        "print(\"Output shape:\", output.shape)  # Expected: (Batch size, Sequence length, vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CH3QSAum65w",
        "outputId": "19b4215b-0945-4381-b6bc-31245b36c366"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (2, 10, 5000)\n"
          ]
        }
      ]
    }
  ]
}